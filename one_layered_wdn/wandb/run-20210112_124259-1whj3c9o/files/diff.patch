diff --git a/one_layered_wdn/main.py b/one_layered_wdn/main.py
index 9e8d7a0..50877e8 100644
--- a/one_layered_wdn/main.py
+++ b/one_layered_wdn/main.py
@@ -18,6 +18,7 @@ import one_layered_wdn.svm as svm
 from torch import nn
 from kmeans_pytorch import kmeans, kmeans_predict
 import copy
+import wandb
 
 # import wandb
 
@@ -172,12 +173,12 @@ def calculate_average_accuracy_over_clusters(train_predictions, test_predictions
                                                                   )
         big_cnnc = FashionCNN()
         cnnc_optimizer = torch.optim.Adam(big_cnnc.parameters(), lr=1e-3, amsgrad=False)
-        train_classifier(big_cnnc, cnnc_optimizer, cluster_cnn_train_dataloader, cluster_cnn_test_dataloader, [], 10)
+        train_classifier(big_cnnc, cnnc_optimizer, cluster_cnn_train_dataloader, cluster_cnn_test_dataloader, [], 1)
         for param in big_cnnc.parameters():
             param.requires_grad = False
 
     for cluster_id in range(n_clusters):
-    # for cluster_id in range(0, 1):
+    # for cluster_id in range(16, 17):
         print("Current cluster ", cluster_id)
         train_cluster_idx = []
         for i in range(len(train_predictions)):
@@ -188,7 +189,7 @@ def calculate_average_accuracy_over_clusters(train_predictions, test_predictions
 
         cluster_cnn_train_dataloader = torch.utils.data.DataLoader(
             train_data,
-            batch_size=min(200, len(train_cluster_idx)),
+            batch_size=min(100, len(train_cluster_idx)),
             # batch_size=len(train_cluster_idx),
             shuffle=False,
             sampler=SubsetRandomSampler(train_cluster_idx)
@@ -211,28 +212,28 @@ def calculate_average_accuracy_over_clusters(train_predictions, test_predictions
         big_cnnc_clone = FashionCNN()
         big_cnnc_clone.load_state_dict(copy.deepcopy(big_cnnc.state_dict()))
 
-        for p in big_cnnc_clone.parameters():
-            p.requires_grad = False
+        # for p in big_cnnc_clone.parameters():
+        #     p.requires_grad = False
 
-        for p in big_cnnc_clone.conv1.parameters():
-            p.requires_grad = True
-        for p in big_cnnc_clone.conv1_bn.parameters():
-            p.requires_grad = True
-        #
-        for p in big_cnnc_clone.conv2.parameters():
-            p.requires_grad = True
-        for p in big_cnnc_clone.conv2_bn.parameters():
-            p.requires_grad = True
+        # for p in big_cnnc_clone.conv1.parameters():
+        #     p.requires_grad = True
+        # for p in big_cnnc_clone.conv1_bn.parameters():
+        #     p.requires_grad = True
+        # #
+        # for p in big_cnnc_clone.conv2.parameters():
+        #     p.requires_grad = True
+        # for p in big_cnnc_clone.conv2_bn.parameters():
+        #     p.requires_grad = True
 
-        for p in big_cnnc_clone.conv3.parameters():
-            p.requires_grad = True
-        for p in big_cnnc_clone.conv3_bn.parameters():
-            p.requires_grad = True
+        # for p in big_cnnc_clone.conv3.parameters():
+        #     p.requires_grad = True
+        # for p in big_cnnc_clone.conv3_bn.parameters():
+        #     p.requires_grad = True
 
-        for p in big_cnnc_clone.conv4.parameters():
-            p.requires_grad = True
-        for p in big_cnnc_clone.conv4_bn.parameters():
-            p.requires_grad = True
+        # for p in big_cnnc_clone.conv4.parameters():
+        #     p.requires_grad = True
+        # for p in big_cnnc_clone.conv4_bn.parameters():
+        #     p.requires_grad = True
 
         # for p in big_cnnc_clone.fc1.parameters():
         #     p.requires_grad = True
@@ -242,15 +243,19 @@ def calculate_average_accuracy_over_clusters(train_predictions, test_predictions
         #
         # for p in big_cnnc_clone.fc3.parameters():
         #     p.requires_grad = True
+        #
+        # for p in big_cnnc_clone.fc4.parameters():
+        #     p.requires_grad = True
 
         # big_cnnc_clone.fc3 = nn.Linear(big_cnnc_clone.fc3.in_features, big_cnnc_clone.fc3.out_features).cuda()
 
         cnnc_optimizer = torch.optim.Adam(big_cnnc_clone.parameters(), lr=1e-4, amsgrad=False)
         # cnnc_optimizer = torch.optim.SGD(big_cnnc_clone.parameters(), lr=1e-3)
-        train_classifier(big_cnnc_clone, cnnc_optimizer, cluster_cnn_train_dataloader, cluster_cnn_test_dataloader, accuracies, 5)
+        train_classifier(big_cnnc_clone, cnnc_optimizer, cluster_cnn_train_dataloader, cluster_cnn_test_dataloader, accuracies, 1)
         print("General Classifier:")
         predict_classifier(big_cnnc, cluster_cnn_test_dataloader, [])
 
+        big_cnnc_clone.cpu()
         del big_cnnc_clone
 
     print("Average accuracy over {} clusters is {}".format(n_clusters, np.sum(accuracies)))
@@ -262,24 +267,24 @@ def train_knn(train_features, test_features, n_clusters):
     # device = torch.device('cpu')
     tr_features = torch.tensor(train_features, dtype=torch.float)
 
-    # tr_features -= tr_features.min(1, keepdim=True)[0]
-    # tr_features /= tr_features.max(1, keepdim=True)[0]
+    tr_features -= tr_features.min(0, keepdim=True)[0]
+    tr_features /= tr_features.max(0, keepdim=True)[0]
 
     te_features = torch.tensor(test_features, dtype=torch.float)
-
-    # te_features -= te_features.min(1, keepdim=True)[0]
-    # te_features /= te_features.max(1, keepdim=True)[0]
+    #
+    te_features -= te_features.min(0, keepdim=True)[0]
+    te_features /= te_features.max(0, keepdim=True)[0]
 
     cluster_ids_x, cluster_centers = kmeans(
         X=tr_features, num_clusters=n_clusters,
-        distance='euclidean',
-        # distance='cosine',
+        # distance='euclidean',
+        distance='cosine',
         device=device
     )
 
     cluster_ids_y = kmeans_predict(te_features, cluster_centers,
-                                   distance='euclidean',
-                                   # distance='cosine',
+                                   # distance='euclidean',
+                                   distance='cosine',
                                    device=device)
 
     # kmeans = KMeans(n_clusters=n_clusters, random_state=0, max_iter=100, algorithm='elkan', n_jobs=-1).fit(
@@ -289,12 +294,30 @@ def train_knn(train_features, test_features, n_clusters):
 
     return cluster_ids_x, cluster_ids_y
 
+def print_cluster_ids(cluster_ids, data_labels):
+    bins = np.zeros((n_clusters, 10))
+    for i in range(len(cluster_ids)):
+        cluster = cluster_ids[i]
+        bins[cluster][int(data_labels[i])] += 1
+    bin_counter = 0
+    for bin in bins:
+        bin_string = ''
+        for amount in bin:
+            a_size = len(str(int(amount)))
+            for i in range(a_size, 6):
+                bin_string += ' '
+            bin_string += str(int(amount))
+        print(bin_counter, bin_string)
+        bin_counter += 1
+
 
 if __name__ == "__main__":
     torch.manual_seed(0)
     np.random.seed(0)
 
-    # wandb.init(project="wdn-v1")
+    wandb.init(project="wdn-v1")
+
+    # config = wandb.config
 
     config = configparser.ConfigParser()
     config.read('config.ini')
@@ -352,16 +375,16 @@ if __name__ == "__main__":
     test_labels = np.load('3_level_test_labels.npy')
 
     n_clusters = 10
-    # print("Fit KNN")
-    # cluster_ids_x, cluster_ids_y = train_knn(train_features, test_features, n_clusters)
+    print("Fit KNN")
+    cluster_ids_x, cluster_ids_y = train_knn(train_features, test_features, n_clusters)
     #
-    # np.save("3_level_train_clusters_10.npy", cluster_ids_x)
-    # np.save("3_level_test_clusters_10.npy", cluster_ids_y)
+    np.save("3_level_train_clusters_10_cosine.npy", cluster_ids_x)
+    np.save("3_level_test_clusters_10_cosine.npy", cluster_ids_y)
 
-    cluster_ids_x = np.load("3_level_train_clusters_10.npy")
-    cluster_ids_y = np.load("3_level_test_clusters_10.npy")
+    # cluster_ids_x = np.load("3_level_train_clusters_10_cosine.npy")
+    # cluster_ids_y = np.load("3_level_test_clusters_10_cosine.npy")
     print("Train predictor")
-    calculate_average_accuracy_over_clusters(cluster_ids_x, cluster_ids_y, 10)
+    calculate_average_accuracy_over_clusters(cluster_ids_x, cluster_ids_y, n_clusters)
     #
     # print("Creating dataset of images")
     # train_dataset = UnsupervisedVectorDataset(train_features, train_labels)
@@ -376,15 +399,11 @@ if __name__ == "__main__":
     # #
     # train_classifier(fcnc, fcnc_optimizer, train_dataset_loader, test_dataset_loader, [])
 
-    # train_bins = np.zeros((n_clusters, 10))
-    # for i in range(len(cluster_ids_x)):
-    #     cluster = cluster_ids_x[i]
-    #     train_bins[cluster][int(train_labels[i])] += 1
-    # bin_counter = 0
-    # for bin in train_bins:
-    #     print(bin_counter, np.array(bin, dtype=np.int))
-    #     bin_counter += 1
-    #
+    print_cluster_ids(cluster_ids_x, train_labels)
+    print("_______________-")
+    print_cluster_ids(cluster_ids_y, test_labels)
+
+
     # test_bins = np.zeros((n_clusters, 10))
     # for i in range(len(cluster_ids_y)):
     #     cluster = cluster_ids_y[i]
